---
title: "Computation Induced Exhaustion: The Impact of Computers"
description: "Despite how important computers are, they're a bit of a problem for our climate."
date: 2022-07-31
tags:
  - "Technology"
  - "Academic-Writing"
---

{% info %} This is an extended version of the essay published in the University of Lethbridge Climate Justice Working Group's _Earth Day 2022_ Zine. Special thanks to both Heather Kehoe and Meghan Rennie for their editorial comments on that original essay. {% endinfo %}

Computers have become an essential part of our lives. We use them to work, communicate, learn and play. With them, we were able to get through the restrictions put in place during the pandemic. Outside the consumer sphere, the industrial, scientific and enterprise uses of computers are astonishing. The field of robotics has exploded as innovations in computing and artificial intelligence make robots more efficient and accurate than humans at certain tasks. Enterprises utilize huge data centers to provide services across the globe. Projects like Folding@home allow consumers to provide their computer's processing power to a network of computers, all working to run protein simulations.

The field of computing is amazing but, as the rise of cryptocurrencies have brought to our attention, it's also power demanding. With an energy usage of around 204.5 terawatt-hours (TWh) per year, the Bitcoin network is equivalent to Thailand in its levels of energy consumption [@digiconomistBitcoinEnergyConsumption]. While this power usage is concerning, the rest of the internet isn't much better. China's data centers consumed 161TWh back in 2018, with estimates assuming their growth would reach 267TWh in the following 5 years [@stanwayChinaInternetData2019]. Back in March 2020, Folding@home reached 470 PetaFLOPS of computational power [@alcornFoldingHomeNow2020], which while less than half the theorized 1039.79 PetaFLOPS of Bitcoin [@silukBitcoinNetworkOutmuscles2013] still puts the potential energy usage of the network at around 90TWh. All this to say computers are a major energy consumer.

There are varying reasons as to why computers utilize so much power, many of which expand beyond computers themselves and bring up more general questions about the impact computers have had on the environment. In short, it comes down to the production of hardware, the use of software, and even its required footprint of land.

## Hardware

From the very start, computers leave an environmental impact from the manufacturing of their components. During the design phase, manufactures have to find a balance between performance and power consumption. Depending on their goals, the manufacturer may choose to prioritize one area over another, like prioritizing performance at the cost of increased power consumption. With multiple parts in a system designed similarly, the power requirements add up. As an extreme example, consider the Intel Core i9-10900K; a power hungry CPU which consumes 129.6 Watts (W) with its base clock speed of 3.7GHz. When over-clocking the chip to 5.2GHz, the required wattage skyrockets to 316.8W [@burkeIntelCoreI910900K]. While this is an unrealistic scenario for normal operation, it is possible to occasionally reach similar clock speeds in regular workloads with Intel's turbo boosting; driver software that pushes the chip harder in high workload scenarios. With that said, it's worth mentioning that the trade-off between power and performance may be worthwhile depending on a system's use case. The most manufactures can do is produce products that are as energy efficient as possible.

Every computer part requires the manufacturing of semiconductor dies, which are produced from pure silicon wafers [@linustechtipsCanNowIntel2022; @amiriHowCPUMade2013] processed in massive fabrication labs. For reference, Intel's fabrication lab in Israel spans 90,000ft^2^, with plans to double the site underway as of writing. Every square inch of space is used for silicon production, with thousands – or potentially millions – of dies being produced at once. Not every die will be used, however, as “it is […] unheard of to get an entire 300 millimeter wafer through the fab without a single defective dye on it [@linustechtipsCanNowIntel2022].”

During production, water is used to keep the silicon clean from any particles that may cause those defects. With how many wafers a fab is processing at one time, however, that means a lot of water is being used. The creation of one wafer can use around 2,200 gallons of water, with 1,500 gallons consisting of Ultra Pure Water which, according to China Water Risk, “takes roughly 1,400–1,600 gallons of municipal water” to make 1,000 gallons of UPW. Luckily, much of the water is recycled and reused, with silicon manufacture TSMC reporting it recycled 87% of its used water in 2019 [@barrettTaiwanDroughtExposing]. What can't be recycled is the 30–50 megawatts of peak electrical capacity that fabs utilize [@chinawaterriskThingsYouShould2013].

Even beyond silicon, the manufacturing of mechanical computer components can be a wasteful process. For example, copper heat pipes are used to assist in the cooling of the CPU and GPU, “sinking and dissipating heat” away from the silicon components [@burkeHowCopperHeatpipes2019]. Much of the manufacturing process creates waste, however, with about 3% out of a daily 50,000 units unable to be recycled due to becoming impure during the manufacturing process [@burkeHowCopperHeatpipes2019].

## Software

Depending on the task, software can also push silicon to its limit. Returning to Bitcoin, the network's power draw comes from the computational difficulty it places in mining. As the Bitcoin network grows, it's designed to make the “Proof of Work” puzzles that make up mining operations more difficult, specifically so that the minting of a new coin is steady at 10 minutes [@pipkinENVIRONMENTALISSUESCRYPTOART2021]. Bitcoin is an example of both high computational load and software inefficiency; designing the code in a way that requires more computational performance than is needed. The former is more applicable to other programs, especially Folding@home in Bitcoin's case. If a user is compiling data, rendering or running a simulation, that is going to put a major computational load on the CPU that's going to require a higher clock speed and thus more power.

Even with powerful components, a workload may still call for more performance. As mentioned previously, CPU's may have software that pushes the chip's clock speed higher for a limited period. It's designed to help with small workloads that are resource intensive, allowing them to be completed quicker. When a chip is over-clocked, that burst speed becomes the default speed. Software such as _MSI Afterburner_ allows for easy over-clocking of a user's hardware, enabling them to squeeze out more computational power. This comes with the price of – as mentioned with the i9-10900K – higher wattage, but also greater system instability.

Instability ranges from lower performance incurred through self-protective thermal throttling measures to system crashes, and even hardware damage. An interesting example of the ladder is EVGA's RTX 3090's, which users were reporting became defective once they attempted to play the closed beta for Amazon Games' _New World_. While the problem was caused by shoddy soldering [@dexterEVGAExplainsWhy2021], it was revealed by an unlimited frame cap in _New World_'s menus. In the initial reports, Dave James commented that “This isn't the first time something like this has happened: Starcraft II was another culprit back in the day.”

In a way, Bitcoin is an analogy for our interaction with technology. Its continual increase in mining difficulty replicates our regular interactions with computers; as we create more powerful and efficient computers, we push them to do more.

## Data Centers

The issues found with hardware and software get compounded onto each other within data centers; buildings that can house hundreds of powerful computers. The computers within data centers can be used in multiple ways: they could be a server for internet traffic, or they can communicate together to function as one supercomputer. There are also data centers built solely to hosting the networking equipment needed for the internet to function. Not taking into account the potential workload that any server may be under, those items require a lot of electricity to keep operational 24/7. That power requirement grows with the need for cooling, security, and redundant power, all three of which take extreme measures given the environment.

A South Carolina data center operated by Google uses multiple levels of physical security to protect the physical servers from unauthorized access. As Joe Kava, Google's Vice-President of Data Centers, explains, “So, just to enter this campus, my badge had to be on a pre-authorized access list. Then, to come into the building, that was another layer of security. To get into the secure corridor that leads to the data center, […] One, my badge has to be on the authorized list. And then two, I use a biometric iris scanner to verify that it is truly me [@googleGoogleDataCenter2014].” While Google's security measures may not be without warrant, it still contributes to the massive power draw of the data center.

European Data Hub Luxembourg is similarly energy dependent with the energy consumption of a small city [@wargamingeuropeWorldWarshipsData2016], and is fully aware of the possible detriment of a loss of power. To combat this, the site takes multiple measures, including receiving power from two different parts of the country that each half of the complex runs off. Ralph Birch, Technical Director, elaborates “All the electrical and cooling services provided on side A are mirrored on side B, and they are completely independent [@wargamingeuropeWorldWarshipsData2016].” Should one power supply go down the other would provide more energy to compensate, but what should happen in the case of a complete power outage? Uninterrupted Power Supplies kick in momentarily to keep the site running while two generators kick in, which are each connected to a 50,000 litter tank of diesel. This allows the site to run for five and a half days, and as Ralph mentions “We are third in the queue [for emergency fuel delivery], after the military, hospitals, and then the data center [@wargamingeuropeWorldWarshipsData2016].”

Finally, data centers must address the complex question of cooling the hardware. As Joe comments, “In the time that I've been at Google – for almost six and a half years now – we have changed our cooling technologies at least five times [@googleGoogleDataCenter2014].” Ralph explained European Data Hub's approach as “using chilled water and pumps, all computer controlled, which then push chilled water around through into the cooling units in each room to take the heat away from the data center.”

This is all without addressing the amount of space required for these data centers. Alongside being 22 meters underground, the total area of European Data Hub's data center is 15,000 meters^2^ [@wargamingeuropeWorldWarshipsData2016]. These buildings might be more aesthetically pleasing than the cargo containers crypto miners retrofit for mining operations [@pipkinENVIRONMENTALISSUESCRYPTOART2021], but they function in much of the same way.

## Conclusion

Taking a step back, it’s astonishing how resource intensive computers are. Their production produces significant amounts of waste, their software pushes the hardware to utilize more energy, and the scale of data centers consumes an astonishing amount of electricity. Making matters worse, our electrical grid still runs primarily off fossil fuels and our interactions with computers are looking to become more demanding in the future.

If we want to improve the sustainability of our planet, it's important that we look at the field of computing and see how we can make it more efficient. This is a task that's already started on multiple fronts. Regulators in both the US and Europe have put standards into law regarding power usage and efficiency for power supplies. Regarding data centers, Microsoft's testing of hosting them underwater has raised promising results, with reliability 8 times greater than a similar data center on land [@roachMicrosoftFindsUnderwater]. There's a lot of work that still needs to be done, but it's worth it if what we do now prevents a forced shutdown in the future.

[bibliography]
